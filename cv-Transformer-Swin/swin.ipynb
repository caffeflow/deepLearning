{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('pt': conda)"
  },
  "interpreter": {
   "hash": "23aa488acc9b8493c9b85b129ce536e22d714f711688d0019e91551159d87651"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from timm.models.layers import DropPath, to_2tuple,trunc_normal_\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "img -> b,inc,h,w: [1, 3, 12, 12]\n"
     ]
    }
   ],
   "source": [
    "class Args():\n",
    "    ...\n",
    "args = Args()\n",
    "\n",
    "args.x = args.img = torch.randn((1,3,12,12))\n",
    "print(\"img -> b,inc,h,w:\",list(args.img.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "patch_embed -> b,num_patches,embed_dim: [1, 9, 6]\n"
     ]
    }
   ],
   "source": [
    "# patch_embed  from b,inc,h,w -> b,num_patches,embed_dim\n",
    "args.embed_dim = 6 # default: 96\n",
    "args.patch_size = 4 # default: 4\n",
    "args.patch_resolution = [args.img.shape[2] // args.patch_size , args.img.shape[3] // args.patch_size] \n",
    "args.num_patches = args.patch_resolution[0] * args.patch_resolution[1]\n",
    "args.norm_layer = nn.LayerNorm # default = nn.LayerNorm . \n",
    "if args.norm_layer is None:\n",
    "    args.norm_layer = nn.Sequential() \n",
    "def patch_embed(args):\n",
    "    x = nn.Conv2d(in_channels=args.img.shape[1], \n",
    "                out_channels= args.embed_dim,\n",
    "                kernel_size=args.patch_size,\n",
    "                stride=args.patch_size)(args.img)\n",
    "    x = torch.flatten(x,start_dim=2).transpose(1,2)\n",
    "    x = args.norm_layer(x.shape[1:])(x) # 在通道上做归一化,即在num_patches维度上做归一化\n",
    "    return x\n",
    "args.x = args.patch_embed = patch_embed(args)\n",
    "\n",
    "assert args.num_patches == args.patch_embed.shape[1]\n",
    "print(\"patch_embed -> b,num_patches,embed_dim:\",list(args.patch_embed.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# absolute pos embed\n",
    "args.ape = False # dafault = False\n",
    "def abs_pos_embed(args):\n",
    "    return trunc_normal_(nn.Parameter(torch.zeros(1,args.num_patches,args.embed_dim)),std=.02)\n",
    "    \n",
    "if args.ape:\n",
    "    args.x = args.x + abs_pos_embed(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos drop\n",
    "args.drap_rate = 0. # defatult 0\n",
    "args.x = args.pos_drop = nn.Dropout(p=args.drap_rate)(args.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Args' object has no attribute 'pos_drop_rate'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_p/6r6jhzss3pvdvzfx9vh7fws40000gn/T/ipykernel_56658/1570646914.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;31m# default = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;31m# default = False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_drop_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# stochastic depth decay relu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Args' object has no attribute 'pos_drop_rate'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - \\\n",
    "            coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(\n",
    "            1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - \\\n",
    "            1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\",\n",
    "                             relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C //\n",
    "                                  self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # make torchscript happy (cannot use tensor as tuple)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(\n",
    "            2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N,\n",
    "                             N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size,\n",
    "               W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous(\n",
    "    ).view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size,\n",
    "                     window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "args.depths = [1,1,2,1] # default = [2,2,6,2]\n",
    "args.num_heads = [1,1,1,1,] # defautl = [3,6,12,24]\n",
    "args.window_size = 7 # default = 7\n",
    "args.mlp_ratio = 4. # default = 4.\n",
    "args.qkv_bias = True # default = True\n",
    "args.qkv_scale = None # default = None\n",
    "args.attn_drop_rate = 0. # default = 0.\n",
    "args.drop_path_rate = 0.1 # default = 0.1\n",
    "args.patch_norm = True # default = True\n",
    "args.use_checkpoint = False # default = False\n",
    "dpr = [x.item() for x in torch.linspace(0,args.drop_path_rate,sum(args.depths))] # stochastic depth decay relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i_layer in range(len(args.depths)):\n",
    "    \"\"\"one BasicLayer\"\"\"\"\n",
    "    dim = int(args.embed_dim * 2 ** i_layer)\n",
    "    input_resolution = (args.patch_resolution[0] // (2**i_layer),args.patch_resolution[1] // (2**i_layer))\n",
    "    depth = args.depths[i_layer]\n",
    "    num_heads = args.num_heads[i_layer]\n",
    "    drop_path = dpr[sum(args.depths[:i_layer]):sum(args.depths[:i_layer+1])]\n",
    "    downsample = True if (i_layer < len(args.depths) -1 ) else False\n",
    "\n",
    "    for i in range(depth):\n",
    "        \"\"\"SwinTransformerBlock\"\"\"\n",
    "        shift_size = 0 if (i%2==0) else args.window_size // 2\n",
    "        drop_path_i = drop_path[i] if isinstance(drop_path,list) else drop_path\n",
    "        if min(args.input_resolution) <= args.window_size:\n",
    "            shift_size = 0 # don't partition windows\n",
    "            args.window_sise = min(args.input_resolution)\n",
    "        assert 0 <= shift_size <= args.window_size,\"shift_size must in 0-window_size\"\n",
    "\n",
    "        H,W = input_resolution\n",
    "        B,L,C = args.x.shape\n",
    "        assert L == H*W, \"input feature has wrong size\"\n",
    "        shorcut = x = args.x\n",
    "        x = args.norm_layer(x.shape[1])(x).view(B,H,W,C)\n",
    "\n",
    "        \"\"\"W-WSA/SW-WSA\"\"\"\n",
    "\n",
    "        # cyclic shift\n",
    "        shift_x = x if shift_size == 0 else torch.roll(x,shifts=(-shift_size,-shift_size),dims=(1,2))\n",
    "\n",
    "        # partition windows -> (num_windows*B, window_size, window_size, C) -> (num_windows*B, window_size*window_size, C)\n",
    "        x_windows = window_partition(shift_x,args.window_size).view(-1,args.window_size*args.window_size,C)\n",
    "        # attn mask\n",
    "        if shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = input_resolution  \n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (slice(0, -args.window_size),\n",
    "                        slice(-args.window_size, -shift_size),\n",
    "                        slice(-shift_size, None))\n",
    "            w_slices = (slice(0, -args.window_size),\n",
    "                        slice(-args.window_size, -shift_size),\n",
    "                        slice(-shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            # nW, window_size, window_size, 1\n",
    "            mask_windows = window_partition(img_mask, self.window_size)\n",
    "            mask_windows = mask_windows.view(-1,\n",
    "                                             self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(\n",
    "                attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "        # W-MSA/SW-MSA\n",
    "        # nW*B, window_size*window_size, C\n",
    "        attn_windows = WindowAttention(\n",
    "            dim,window_size=(args.window_size,args.window_size),num_heads=args.num_heads,\n",
    "            qkv_bias=args.qkv_bias,qk_scale=args.qkv_scale,attn_drop=args.attn_drop_rate,proj_drop=args.drop_path)(x_windows,mask = attn_mask)\n",
    "\n",
    "        # merge windows \n",
    "        attn_windows = attn_windows.view(-1,args.window_size,args.window_size,C)\n",
    "        shifted_x = window_reverse(attn_windows,args.window_size,H,W) # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(shift_size,shift_size), dim=(1,2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B,H*W,C)\n",
    "\n",
    "        # FFN\n",
    "\n",
    "        x = shortcut + drop_path(x,drop_path_i,training=True)\n",
    "        mlp_x = nn.Sequential(\n",
    "            nn.Linear(dim,int(dim*args.mlp_ratio)),\n",
    "            nn.GELU()\n",
    "            nn.Linear(int(dim*args.mlp_ratio),dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(args.drop_rate)\n",
    "        )(args.norm_layer(x))\n",
    "        x = x + drop_path(mlp_x)\n",
    "\n",
    "    if args.downsample:\n",
    "        x = donwsample(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}